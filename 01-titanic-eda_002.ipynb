{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic3 dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thomas Cason from the University of Virginia has greatly updated and improved the titanic data frame using the Encyclopedia Titanica and created a new dataset called TITANIC3. This dataset reflects the state of data available as of August 2, 1999. Some duplicate passengers have been dropped; many errors have been corrected; many missing ages have been filled in; and new variables have been created.\n",
    "\n",
    "https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3info.txt\n",
    "\n",
    "#https://biostat.app.vumc.org/wiki/Main/DataSets\n",
    "#https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "import pandas as pd  # data tools\n",
    "import numpy as np  # maths\n",
    "import seaborn as sns # visualizations\n",
    "import missingno as msno # for NaN visualization\n",
    "import matplotlib.pyplot as plt # for data visualization, graph plotting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform\n",
    "import sweetviz as viz\n",
    "\n",
    "#from scipy.stats import randint, uniform\n",
    "#from sklearn import linear_model,preprocessing,tree,model_selection # for prediction models\n",
    "# from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# #from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "#.................................\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data set variables and their locations.\n",
    "\n",
    "data_folder = './data/titanic3.xls'\n",
    "\n",
    "data = pd.read_excel(data_folder)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, test = train_test_split(data, test_size=.2, random_state=15)\n",
    "\n",
    "print(\"features train and test: \", X_train.shape, X_test.shape)\n",
    "print('targets train and test: ', y_train.shape, y_test.shape)\n",
    "\n",
    "report = viz.compare([X_train,\"train\"], [X_test, \"test\"],)\n",
    "\n",
    "report.show_html(\"Report.html\") # Not providing a filename will default to SWEETVIZ_REPORT.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the header records for the datasets, what kind of data is there.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis - the cabin number has some NAN which, if we use, will need to be dealth with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "VARIABLE DESCRIPTIONS:\n",
    "| pclass                                             | Passenger Class   |  (1 = 1st; 2 = 2nd; 3 = 3rd)        |\n",
    "| :---------------------------------------------------------------------------------------------------------- | --- | --- |\n",
    "| survival   |  Survival     |   (0 = No; 1 = Yes)   |\n",
    "| name          | name    |     |\n",
    "| sex           | sex    |     |\n",
    "| age                  | age    |     |\n",
    "| sibsp                | Number of Siblings/Spouses Aboard    |     |\n",
    "| parch                | Number of Parents/Children Aboard     |     |\n",
    "| ticket               | Ticket Number     |     |\n",
    "| fare                 | Passenger Fare     |     |\n",
    "| cabin                | Cabin     |     |\n",
    "| embarked                  | Port of Embarkation    | (C = Cherbourg; Q = Queenstown; S = Southampton)    |\n",
    "| boat                | Lifeboat     |     |\n",
    "| body                | Body Identification Number    |     |\n",
    "| home.dest           | Home/Destination    |     |\n",
    "\n",
    "SPECIAL NOTES:\n",
    "Pclass is a proxy for socio-economic status (SES)\n",
    "1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n",
    "\n",
    "Age is in Years; Fractional if Age less than One (1)\n",
    "If the Age is Estimated, it is in the form xx.5\n",
    "\n",
    "Fare is in Pre-1970 British Pounds (£)\n",
    "Conversion Factors:  1£ = 12s = 240d and 1s = 20d\n",
    "\n",
    "With respect to the family relation variables (i.e. sibsp and parch) some relations were ignored.  The following are the definitions used for sibsp and parch.\n",
    "\n",
    "Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n",
    "Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiancées Ignored)\n",
    "Parent:   Mother or Father of Passenger Aboard Titanic\n",
    "Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n",
    "\n",
    "Other family relatives excluded from this study include cousins,\n",
    "nephews/nieces, aunts/uncles, and in-laws.  Some children travelled\n",
    "only with a nanny, therefore parch=0 for them.  As well, some\n",
    "travelled with very close friends or neighbors in a village, however,\n",
    "the definitions do not support such relations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age runs from a few months to 80 years, siblings/spouse from 0 to 8, parents/children from 0 to 6 and fare from 0 to 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking on if/where the null values are\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cabin is missing 77% of their values and age is missing 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(data)  # taking a look at the missing data in relation to the other variables in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing data in each Titanic dataframe column:')\n",
    "for c in data.columns:\n",
    "    missing_data = len(data) - data[c].count()\n",
    " \n",
    "    if (missing_data > 0 or missing_data =='NaN'):        \n",
    "        print(f\"    {c} is missing {str(round(float(missing_data / float(len(data))) * 100, 1))}% if its values ({missing_data})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis of the missing data:\n",
    "\n",
    "    1: age is missimg 20% of its values.  I need to see if thiss can be supplemented because this would be aa desireable feature in survavbility analysis.\n",
    "    2: cabin is mostly missing its values but the deck or general location of the cabin might be able to be inferred by the fare which is missing only 1 value    \n",
    "    3: body number are mostly missing their data.  With over 50% of their data missing and unless this can be supplemented, its data will be dropped.\n",
    "    4: boat is missing 63% of its data, let's see if this can be supplemented in some manner to see if it adds anything to the prediction.  \n",
    "    4: home/destination is missing 43% of its data, we'll keep it in the dataset so othe ranalysis can be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num = data.select_dtypes(exclude=\"object\")\n",
    "cols_cat = data.select_dtypes(include=\"object\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = {col:data[col].nunique() for col in cols_cat}  # retrieve unique values for each of the categeorical columns\n",
    "for key in uniques:\n",
    "    print(f\"{key} has {uniques[key]} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age is usually an important indicator, we should impute the missing values for age:\n",
    "# copy the data: \n",
    "\n",
    "titanic = data.copy()\n",
    "titanic['age'] = titanic['age'].fillna(titanic['age'].mean())\n",
    "titanic.head()\n",
    "\n",
    "# there are other ways that this could have been solved but due to the small dataset size, getting more precise here would likely have little return value in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cabin is missing 77% of its values.  one method to solve IS to use the fare column.  more than likely the fare across cabins was similar\n",
    "\n",
    "# create a new dataframe which only includes the available cabin numbers\n",
    "cabinfare = titanic[['cabin', 'fare']]\n",
    "cabinfare = cabinfare[cabinfare['cabin'].notna()]\n",
    "cabinfare.reset_index(drop=True, inplace=True)\n",
    "#cabinfare.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ''\n",
    "\n",
    "for i in range(0, (len(titanic.cabin))):\n",
    "    cabin = titanic.cabin[i]\n",
    "    \n",
    "    if str(cabin) == \"nan\":\n",
    "        val = cabinfare.iloc[(cabinfare['fare']-titanic.fare[i]).abs().argsort()[:1]]  # the :1 says send me back the closest record\n",
    "        titanic.at[i,'cabin'] = str(val.iat[0,0])\n",
    "\n",
    "regex = \"(?P<deck>[A-Z])(?P<Number>[0-9]*)\"\n",
    "tmp = titanic['cabin'].str.extract(regex, expand=False)\n",
    "titanic['deck'] = tmp['deck'].fillna('Z')\n",
    "\n",
    "print(f\"Unique FN: {titanic['deck'].unique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next is to fix of 'embarked'\n",
    "# 1) since most of the passengers embarked in southhampton, we should fill the emties in with 'S'%%!\n",
    "# 2) drop the boat and body columns, too much missing data in each\n",
    "\n",
    "titanic[\"embarked\"].fillna(\"S\", inplace = True)\n",
    "titanic.drop(['boat', 'body', 'name', 'ticket', 'fare', 'cabin', 'home.dest'], axis=1, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the keys to make them more descriptive\n",
    "\n",
    "def label_survive(row):\n",
    "    if row['survived'] == 1:\n",
    "        return 'yes'\n",
    "    if row['survived'] == 0:\n",
    "        return 'no'\n",
    "    else:    \n",
    "        return 'unk'\n",
    "\n",
    "        \n",
    "def label_familysize(row):\n",
    "    if row['sibsp'] + row['parch'] == 0:\n",
    "        return 0\n",
    "    if row['sibsp'] + row['parch'] == 1:\n",
    "        return 1\n",
    "    if row['sibsp'] + row['parch'] == 2:\n",
    "        return 2\n",
    "    else:    \n",
    "        return 3\n",
    "\n",
    "# embarked keys:\n",
    "titanic.loc[:,'embarked'].replace(['C','S','Q'], ['Cherbourg','Southampton','Queenstown'], inplace=True)\n",
    "\n",
    "# survived keys:\n",
    "#titanic.loc[:,'survived'].replace([0,1],['no','yes'], inplace=True)\n",
    "titanic['survived label'] = titanic.apply(lambda row: label_survive(row), axis = 1)\n",
    "titanic['companions'] = titanic.apply(lambda row: label_familysize(row), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at age in detail\n",
    "# Get summary descriptive statistics\n",
    "df_describe = pd.DataFrame(titanic['age'].describe())\n",
    "\n",
    "#Change the index labels and round the values reported\n",
    "df_describe.index = ['population size', 'mean', 'std. dev', 'min', '25% qt', 'median', '75% qt', 'max']\n",
    "df_describe = df_describe.round(decimals=3)\n",
    "df_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=( 12, 6))\n",
    "# sns.histplot(x= titanic['age'],  kde=True, ax = axs[0])\n",
    "# sns.violinplot(x= titanic['age'],  ax = axs[1])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=( 25, 6))\n",
    "sns.histplot(x= titanic['age'],  kde=True, ax = axs[0])\n",
    "sns.violinplot(x= titanic['age'],  ax = axs[1])\n",
    "\n",
    "age_bins = np.arange(0, titanic['age'].max()+5, 5) #np.arange(0, 80, 4)\n",
    "sns.histplot(titanic.loc[(titanic['survived label']=='no') & (~titanic['age'].isnull()),'age'], bins=age_bins, color='#d62728',  kde=True,   ax = axs[2])\n",
    "sns.histplot(titanic.loc[(titanic['survived label']=='yes') & (~titanic['age'].isnull()),'age'], bins=age_bins,  kde=True,   ax = axs[2])\n",
    "plt.title('age distribution among survival classes')\n",
    "plt.ylabel('absolute frequency')\n",
    "plt.legend(['did not survive', 'survived'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for the age cohorts ===============\n",
    "bins = [0, 5, 14, 25, 35, 60, np.inf]\n",
    "labels = ['toddler', 'child', 'teenager', 'young adult', 'adult', 'senior']\n",
    "titanic['age cohort'] = pd.cut(titanic[\"age\"], bins, labels = labels)\n",
    "\n",
    "# for the columns being used in the charts\n",
    "cols = ['pclass', 'survived', 'sex',  'sibsp', 'parch', 'embarked', 'deck','age']\n",
    "\n",
    "vz_rows = 2\n",
    "vz_cols = 4\n",
    "\n",
    "fig, axs = plt.subplots(vz_rows, vz_cols, figsize=( vz_cols * 6, vz_rows * 6))\n",
    "\n",
    "for r in range(0, vz_rows):\n",
    "    for c in range(0, vz_cols):\n",
    "        if r==1 and c==3: # draw the last chart differently from the others            \n",
    "            ax = axs[r][c]\n",
    "            sns.barplot(x=\"age cohort\", y=\"survived\", data=titanic, ax = ax)\n",
    "            #ax.set_title(cols[i], fontsize= 14, fontweight= 'bold')\n",
    "            ax.legend(title= 'survival amongst age cohorts', loc= 'upper center')\n",
    "        else:\n",
    "            i = r * vz_cols + c\n",
    "            ax = axs[r][c]\n",
    "            sns.countplot(x= titanic[cols[i]], hue=titanic['survived'],  ax = ax)        \n",
    "            ax.set_title(cols[i], fontsize= 14, fontweight= 'bold')\n",
    "            ax.legend(title= 'survived', loc= 'upper center')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis: for the age groups, the youngest ages had the best chance of survival while adults experienced a better surivial rate than either teenagers or young adults.  \n",
    "\n",
    "The charts above show that a male travelling without family and not in first class would have a poor chance of surviving the sinking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at survivorship amongst various groups\n",
    "# age / pclass\n",
    "# age / sex\n",
    "# pclass / family\n",
    "\n",
    "\n",
    "cols1 = ['pclass', 'sex',  'companions', 'deck', 'pclass', 'sex','deck', 'embarked' ]\n",
    "cols2 = ['age', 'age',  'age', 'age', 'companions', 'companions','companions', 'age']\n",
    "\n",
    "vz_rows = 2\n",
    "vz_cols = 4\n",
    "\n",
    "fig, axs = plt.subplots(vz_rows, vz_cols, figsize=( vz_cols * 6, vz_rows * 6))\n",
    "\n",
    "for r in range(0, vz_rows):\n",
    "    for c in range(0, vz_cols):        \n",
    "        i = r * vz_cols + c\n",
    "        ax = axs[r][c]\n",
    "        sns.violinplot(x= titanic[cols1[i]], y=titanic[cols2[i]], hue=titanic['survived'], split=True, palette={0: 'r', 1: 'g'}, ax = ax)   \n",
    "        #ax.set_title(cols[i], fontsize= 14, fontweight= 'bold')\n",
    "        #ax.legend(title= 'survived', loc= 'upper center')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "#sns.violinplot(x=’Pclass’, y=’Age’, hue=’Survived’, split=True, data=train_data, palette={0: “r”, 1: “g”});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pclass = [1,2,3]\n",
    "# ageLbound = [60, 40, 20]\n",
    "# ageUbound = [99, 60, 40]\n",
    "\n",
    "#ageLbound = [70,65,60,55,50,45,40,35,30,25,20,15,10,5,0]\n",
    "#ageUbound = [99,70,65,60,55,50,45,40,35,30,25,20,15,10,5]\n",
    "\n",
    "ageLbound = [70,60,50,40,30,20,10,0]\n",
    "ageUbound = [99,70,60,50,40,30,20,10]\n",
    "\n",
    "\n",
    "sex = ['male', 'female']\n",
    "\n",
    "df_stat = pd.DataFrame(columns = ['class', 'sex', 'age', 'surv_pct'])\n",
    "\n",
    "def div(x,y):\n",
    "    if y == 0: return 0\n",
    "    return x / y\n",
    "\n",
    "for c in range(0, 3):\n",
    "    for a in range(0, 8):\n",
    "        for s in range(0,2):\n",
    "\n",
    "            rate = round(div(len(titanic[(titanic['pclass']==pclass[c]) & (titanic['age']>ageLbound[a]) & (titanic['age']<=ageUbound[a]) \\\n",
    "                        & (titanic['survived']==1) & (titanic['sex']==sex[s]) ]) , len(titanic[(titanic['pclass']==pclass[c]) & \\\n",
    "                        (titanic['age']>ageLbound[a]) & (titanic['age']<=ageUbound[a]) & (titanic['sex']==sex[s]) ]))*100,1)\n",
    "\n",
    "            df_stat = df_stat.append({'class':pclass[c], 'sex':sex[s], 'age':ageLbound[a], 'surv_pct':rate}, ignore_index=True)\n",
    "            \n",
    "\n",
    "#sns.lineplot(x= 'class', y= 'surv_pct', hue='sex', data=df_stat)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "sns.lineplot( x = 'age',\n",
    "             y = 'surv_pct',\n",
    "             hue = 'sex',\n",
    "             style= 'class',\n",
    "             data = df_stat)\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis: although close to be confusing, the chart above shows a male in the third class, over the age of ten had a low chance to survive the sinking.  a female, above the age of ten and in first class had the best chance to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant features\n",
    "titanic.drop(['age', 'survived label'], axis=1, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the models\n",
    "1) logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = titanic.drop(['survived'], axis=1)\n",
    "target = titanic['survived']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.get_dummies(features)\n",
    "#features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(features.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(features.corr(),\n",
    "            cmap='coolwarm', mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=.2, random_state=15)\n",
    "print(\"features train and test: \", X_train.shape, X_test.shape)\n",
    "print('targets train and test: ', y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report = viz.compare([X_train,\"train\"], [X_test, \"test\"],)\n",
    "\n",
    "# report.show_html(\"Report.html\") # Not providing a filename will default to SWEETVIZ_REPORT.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation \n",
    "\n",
    "Evaluate, using cross-validation, which of the following classifiers performs best with our training data:\n",
    "\n",
    "Decision Tree\n",
    "Random Forest\n",
    "\n",
    "Extra Trees\n",
    "\n",
    "AdaBoost\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "K-nearest neighbors\n",
    "\n",
    "SVC\n",
    "\n",
    "Gradient Boosting\n",
    "\n",
    "eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=15\n",
    "\n",
    "# Scale features such that the mean is 0 and standard deviation is \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Number of cross-validation folds\n",
    "k_folds = 10\n",
    "\n",
    "# Number of estimators for tree-based ensembles\n",
    "n_estimators = 100\n",
    "\n",
    "# Create a dictionary containing the instance of the models, scores, mean accuracy and standard deviation\n",
    "classifiers = {\n",
    "    'name': ['DecisionTree', 'RandomForest', 'ExtraTrees', 'AdaBoost', 'LogReg', 'KNN', 'SVC',\n",
    "             'XGBoost', 'GradientBoost'],\n",
    "    'models': [DecisionTreeClassifier(random_state=random_state),\n",
    "               RandomForestClassifier(random_state=random_state, n_estimators=n_estimators),\n",
    "               ExtraTreesClassifier(random_state=random_state, n_estimators=n_estimators),\n",
    "               AdaBoostClassifier(random_state=random_state, n_estimators=n_estimators),\n",
    "               LogisticRegression(random_state=random_state),\n",
    "               KNeighborsClassifier(),\n",
    "               SVC(random_state=random_state),\n",
    "               XGBClassifier(random_state=random_state, n_estimators=n_estimators),\n",
    "               GradientBoostingClassifier(random_state=random_state, n_estimators=n_estimators)], \n",
    "    'scores': [],\n",
    "    'acc_mean': [],\n",
    "    'acc_std': []\n",
    "}\n",
    "\n",
    "# Run cross-validation and store the scores\n",
    "for model in classifiers['models']:\n",
    "    score = cross_val_score(model, X_train, y_train, cv=k_folds, n_jobs=4)\n",
    "    classifiers['scores'].append(score)\n",
    "    classifiers['acc_mean'].append(score.mean())\n",
    "    classifiers['acc_std'].append(score.std())    \n",
    "\n",
    "# Create a nice table with the results\n",
    "classifiers_df = pd.DataFrame({\n",
    "    'Model Name': classifiers['name'],\n",
    "    'Accuracy': classifiers['acc_mean'],\n",
    "    'Std': classifiers['acc_std']\n",
    "}, columns=['Model Name', 'Accuracy', 'Std']).set_index('Model Name')\n",
    "\n",
    "classifiers_df.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's tune the top three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from script_step2 import train_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "\n",
    "from script_step2 import train_evaluate\n",
    "\n",
    "SPACE = [\n",
    "    skopt.space.Real(0.01, 0.5, name='learning_rate', prior='log-uniform'),\n",
    "    skopt.space.Integer(1, 30, name='max_depth'),\n",
    "    skopt.space.Integer(2, 100, name='num_leaves'),\n",
    "    skopt.space.Real(0.1, 1.0, name='feature_fraction', prior='uniform'),\n",
    "    skopt.space.Real(0.1, 1.0, name='subsample', prior='uniform')]\n",
    "\n",
    "\n",
    "@skopt.utils.use_named_args(SPACE)\n",
    "def objective(**params):\n",
    "    return -1.0 * train_evaluate(params)\n",
    "\n",
    "\n",
    "results = skopt.forest_minimize(objective, SPACE, n_calls=30, n_random_starts=10)\n",
    "best_auc = -1.0 * results.fun\n",
    "best_params = results.x\n",
    "\n",
    "print('best result: ', best_auc)\n",
    "print('best parameters: ', best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Tuning hyperparameters \n",
    "\n",
    "To further improve the models we can tune their hyperparameters using randomized parameter optimization or grid search. \n",
    "\n",
    "I chose randomized parameter optimization because it typically performs just as well as grid search but with much fewer iterations, furthermore, the number of iterations in a parameter that we control. \n",
    "\n",
    "Obviously, more iterations are always better but if we want to make quick tests we can easily reduce the tuning time by lowering this parameter instead of reducing all of the hyperparameter search ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "# Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n",
    "def report(results, n_top=3, limit=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        if limit is not None:\n",
    "            candidates = candidates[:limit]\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.4f} (std: {1:.4f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print()\n",
    "\n",
    "# Number of iterations\n",
    "n_iter_search = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=random_state)\n",
    "rand_param = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': uniform(0.01, 10)\n",
    " }\n",
    "\n",
    "logreg_search = RandomizedSearchCV(logreg, param_distributions=rand_param, n_iter=n_iter_search, cv=k_folds, n_jobs=4, verbose=1)\n",
    "logreg_search.fit(X_train, y_train)\n",
    "report(logreg_search.cv_results_)\n",
    "\n",
    "logreg_best = logreg_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(random_state=random_state, probability=True)\n",
    "rand_param = {\n",
    "    'C': uniform(0.01, 10),\n",
    "    'gamma': uniform(0.01, 10)\n",
    " }\n",
    "\n",
    "svc_search = RandomizedSearchCV(svc, param_distributions=rand_param, n_iter=n_iter_search, cv=k_folds, n_jobs=4, verbose=1)\n",
    "svc_search.fit(X_train, y_train)\n",
    "report(svc_search.cv_results_)\n",
    "\n",
    "svc_best = svc_search.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3879a765c6eb138eec1f0097e8295db54d70e1cb3fa7d10bbbb3b1eb59c42bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
